# Machine-Learning
Malicious Application Detection

Problem Statement
Malware is a software or a piece of code that is hostile and often used to corrupt or misuse a system. These programs have a variety of functionalities, such as theft or deletion of sensitive data, modification or diversion of basic functions of computers and monitoring the activity of the computer without user permission. It can cause the computer to crash or gather personal information or data from your computer. We consider the detection of this malware infected windows executable files as a classification problem and employ machine learning models to detect these infected files. On the dataset we are going to train our machine learning algorithms: K-NN, Random Forest, XGBoost before and after feature selection and analyze the accuracies of the models.

![image](https://github.com/vandana2508/Machine-Learning/assets/59761440/b2b2de56-6798-4213-b95a-e5ef52c2ff18)

Machine Learning Models Implemented-
Classification: A Supervised Learning Approach
Supervised learning is a technique in which the machine is trained with data that is well labelled. The objective of the model is to predict the correct label for the newly presented input data. The problem in
supervised learning is to find an adequate model and its parameterizations. The supervised learning models used on this dataset are K-Nearest Neighbor, Random Forest, XGBoost. To avoid overfitting, we use a selection strategy called cross-validation. The idea of cross-validation is to split up the N observations into training, validation, and test sets. The training set is used as the basis for the learning algorithm given a potential parameter set. The validation set is used to evaluate the model given the parameter set. The optimized model based on a training process with cross-validation is finally evaluated on an independent test set.
1. KNN:
K-nearest neighbors (KNN), is based on the idea that the nearest patterns to a target pattern for which we seek the label, deliver useful label information [14]. KNN locates all of the closest neighbors around the unknown data point, it calculates the distance from all the points and filters out the ones with shortest distances to figure out what class it belongs to. KNN assigns the class label to most of the K-nearest patterns in data space. For this sake, we have to be able to define a similarity measure in data space [6]. We used Minkowski metric computation to classify the k-nearest patterns. The problem in supervised learning is to find an adequate model and its parameterizations [1]. To avoid overfitting, we use a selection strategy called cross-validation. The idea of cross-validation is to split up the N observations into training, validation, and test sets. The training set is used as the basis for the learning algorithm given a potential parameter set. The validation set is used to evaluate the model given the parameter set. The optimized model based on a training process with cross-validation is finally evaluated on an independent test set.
2. XGBoost (eXtreme Gradient Boosting):
XGBoost is a scalable and accurate implementation of gradient amplification machines and has proven to push the limits of computing power for amplified tree algorithms [2], both because it was built and developed for the sole purpose of performance. XGBoost includes efficient linear model solver and tree learning algorithm. XGBoost is robust enough to support fine tuning and adding adjustment parameters [3].
3. Random Forest:
Random forest is an algorithm that learns from a weak model (like DT) to build a more robust one to avoid over-adjustment with a minimum cost [5]. The forest is built using well-known bootstrap techniques [7]. The main idea of bootstrapping is to merge learning models by increasing the overall classification result.
